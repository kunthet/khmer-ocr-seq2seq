ðŸš€ Starting Khmer OCR Training...
Warning: Khmer normalization not available
Warning: Khmer syllable segmentation not available
2025-07-13 10:00:12.468239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1752400812.489252  134511 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1752400812.495746  134511 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-13 10:00:16,303 - KhmerOCROnTheFlyTraining - INFO - ======================================================================
2025-07-13 10:00:16,303 - KhmerOCROnTheFlyTraining - INFO - Khmer OCR Seq2Seq On-the-Fly Training
2025-07-13 10:00:16,303 - KhmerOCROnTheFlyTraining - INFO - ======================================================================
2025-07-13 10:00:16,310 - KhmerOCROnTheFlyTraining - INFO - Using device: cuda
2025-07-13 10:00:16,329 - KhmerOCROnTheFlyTraining - INFO - GPU: NVIDIA A100-SXM4-40GB
2025-07-13 10:00:16,329 - KhmerOCROnTheFlyTraining - INFO - GPU Memory: 39.6 GB
2025-07-13 10:00:16,329 - KhmerOCROnTheFlyTraining - INFO - Found fixed validation set at: data/validation_fixed
2025-07-13 10:00:16,329 - KhmerOCROnTheFlyTraining - INFO - Loading configuration from: configs/train_config.yaml
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO - Training Configuration:
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO -   Epochs: 500
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO -   Learning rate: 1e-06
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO -   Optimizer: adam
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO -   Teacher forcing ratio: 1.0
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO -   Gradient clip: 5.0
2025-07-13 10:00:16,336 - KhmerOCROnTheFlyTraining - INFO -   Training samples per epoch: 128000
2025-07-13 10:00:16,337 - root - INFO - Creating Khmer OCR Seq2Seq model...
2025-07-13 10:00:16,636 - root - INFO - Model created successfully
2025-07-13 10:00:16,637 - root - INFO - Total parameters: 16,327,989
2025-07-13 10:00:16,637 - root - INFO - Trainable parameters: 16,327,989
2025-07-13 10:00:16,637 - KhmerOCROnTheFlyTraining - INFO - Using specified batch size: 128
2025-07-13 10:00:16,637 - root - INFO - Creating datasets...
2025-07-13 10:00:16,637 - src.data.onthefly_dataset - INFO - Found 2 training files: ['train_0.txt', 'train_1.txt']
2025-07-13 10:00:16,823 - src.data.onthefly_dataset - INFO - Loaded 147195 lines from train_0.txt
2025-07-13 10:00:17,035 - src.data.onthefly_dataset - INFO - Loaded 166118 lines from train_1.txt
2025-07-13 10:00:17,160 - src.data.onthefly_dataset - INFO - Loaded 313313 text lines for train split
2025-07-13 10:00:17,164 - src.synthetic_data_generator.khmer_ocr_generator - INFO - Loaded working fonts: ['KhmerOSmuollight', 'KhmerOSbokor', 'KhmerOSfasthand', 'KhmerOS', 'KhmerOSbattambang', 'KhmerOSmuol', 'KhmerOSmetalchrieng', 'KhmerOSsiemreap']
2025-07-13 10:00:17,164 - src.synthetic_data_generator.khmer_ocr_generator - INFO - Khmer OCR Synthetic Generator initialized
2025-07-13 10:00:17,164 - src.synthetic_data_generator.khmer_ocr_generator - INFO - Image height: 32px (variable width)
2025-07-13 10:00:17,164 - src.synthetic_data_generator.khmer_ocr_generator - INFO - Loaded 8 fonts
2025-07-13 10:00:17,164 - src.synthetic_data_generator.khmer_ocr_generator - INFO - Advanced backgrounds: True
2025-07-13 10:00:17,164 - src.synthetic_data_generator.khmer_ocr_generator - INFO - Training augmentation: True
2025-07-13 10:00:17,164 - src.data.onthefly_dataset - INFO - Initialized on-the-fly train dataset:
2025-07-13 10:00:17,164 - src.data.onthefly_dataset - INFO -   Text lines: 313313
2025-07-13 10:00:17,164 - src.data.onthefly_dataset - INFO -   Samples per epoch: 128000
2025-07-13 10:00:17,164 - src.data.onthefly_dataset - INFO -   Augmentation: enabled
2025-07-13 10:00:17,164 - src.data.onthefly_dataset - INFO -   Fonts: 8
2025-07-13 10:00:17,164 - root - INFO - Loading fixed validation set from data/validation_fixed
2025-07-13 10:00:17,217 - src.data.synthetic_dataset - INFO - Loaded 6400 samples from metadata
2025-07-13 10:00:17,218 - src.data.synthetic_dataset - INFO - Loaded 6400 samples for val split from data/validation_fixed
2025-07-13 10:00:17,218 - root - INFO - Created train dataloader: 1000 batches (128000 samples)
2025-07-13 10:00:17,218 - root - INFO - Created val dataloader: 50 batches (6400 samples)
2025-07-13 10:00:17,218 - KhmerOCROnTheFlyTraining - INFO - Initializing trainer...
2025-07-13 10:00:18,414 - CheckpointManager - INFO - Google Drive backup enabled: /content/drive/MyDrive/KhmerOCR_Checkpoints
2025-07-13 10:00:18,419 - KhmerOCRTrainer - INFO - Google Drive backup: âœ… Ready
2025-07-13 10:00:18,419 - KhmerOCRTrainer - INFO - Google Drive dir: /content/drive/MyDrive/KhmerOCR_Checkpoints
2025-07-13 10:00:18,420 - KhmerOCROnTheFlyTraining - INFO - Starting on-the-fly training...
2025-07-13 10:00:18,420 - KhmerOCROnTheFlyTraining - INFO - Training Method: On-the-fly image generation
2025-07-13 10:00:18,420 - KhmerOCROnTheFlyTraining - INFO - Training Samples: 128000 per epoch (unlimited variety)
2025-07-13 10:00:18,420 - KhmerOCROnTheFlyTraining - INFO - Validation Samples: 6400 (fixed set)
2025-07-13 10:00:18,420 - KhmerOCROnTheFlyTraining - INFO - Target: <=1.0% CER (Character Error Rate)
2025-07-13 10:00:18,420 - KhmerOCRTrainer - INFO - Loading checkpoint from /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_001.pth
2025-07-13 10:00:18,616 - CheckpointManager - INFO - Loaded checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_001.pth
2025-07-13 10:00:18,672 - KhmerOCRTrainer - INFO - Resumed from epoch 2
2025-07-13 10:00:18,672 - KhmerOCRTrainer - INFO - Best CER so far: 111.90%
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO - Starting training...
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO - Training configuration:
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO -   Epochs: 500
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO -   Batch size: 128
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO -   Learning rate: 1e-06
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO -   Teacher forcing ratio: 1.0
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO -   Device: cuda
2025-07-13 10:00:18,674 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 10:00:18,675 - KhmerOCRTrainer - INFO - Epoch 3/500
2025-07-13 10:00:18,675 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 10:00:32,348 - KhmerOCRTrainer - INFO - Epoch 2, Batch 0/1000 (0.0%), Loss: 3.6716
2025-07-13 10:03:39,813 - KhmerOCRTrainer - INFO - Epoch 2, Batch 100/1000 (10.0%), Loss: 3.6712
2025-07-13 10:07:00,876 - KhmerOCRTrainer - INFO - Epoch 2, Batch 200/1000 (20.0%), Loss: 3.6824
2025-07-13 10:10:09,673 - KhmerOCRTrainer - INFO - Epoch 2, Batch 300/1000 (30.0%), Loss: 3.6518
2025-07-13 10:13:24,508 - KhmerOCRTrainer - INFO - Epoch 2, Batch 400/1000 (40.0%), Loss: 3.6743
2025-07-13 10:16:38,019 - KhmerOCRTrainer - INFO - Epoch 2, Batch 500/1000 (50.0%), Loss: 3.6605
2025-07-13 10:19:58,436 - KhmerOCRTrainer - INFO - Epoch 2, Batch 600/1000 (60.0%), Loss: 3.6617
2025-07-13 10:23:14,426 - KhmerOCRTrainer - INFO - Epoch 2, Batch 700/1000 (70.0%), Loss: 3.6727
2025-07-13 10:26:33,199 - KhmerOCRTrainer - INFO - Epoch 2, Batch 800/1000 (80.0%), Loss: 3.6855
2025-07-13 10:29:51,943 - KhmerOCRTrainer - INFO - Epoch 2, Batch 900/1000 (90.0%), Loss: 3.6763
Validation progress: 0.0%
2025-07-13 10:33:32,329 - KhmerOCRTrainer - INFO - 
Epoch 3 Results:
2025-07-13 10:33:32,329 - KhmerOCRTrainer - INFO -   Train Loss: 3.6688
2025-07-13 10:33:32,329 - KhmerOCRTrainer - INFO -   Val Loss: 3.6932
2025-07-13 10:33:32,329 - KhmerOCRTrainer - INFO -   Val CER: 111.80%
2025-07-13 10:33:32,329 - KhmerOCRTrainer - INFO -   Epoch Time: 1974.81s
2025-07-13 10:33:32,329 - KhmerOCRTrainer - INFO -   New best CER: 111.80%
2025-07-13 10:33:32,768 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_002.pth
2025-07-13 10:33:33,421 - CheckpointManager - INFO - Saved best model: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/best_model.pth
2025-07-13 10:33:34,068 - CheckpointManager - INFO - Backed up best model to Google Drive: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/best_model.pth
2025-07-13 10:33:34,072 - CheckpointManager - INFO - Saved training history to Google Drive: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/training_history.json
2025-07-13 10:33:34,076 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 10:33:34,076 - KhmerOCRTrainer - INFO - Epoch 4/500
2025-07-13 10:33:34,076 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 10:33:42,431 - KhmerOCRTrainer - INFO - Epoch 3, Batch 0/1000 (0.0%), Loss: 3.6602
2025-07-13 10:36:53,910 - KhmerOCRTrainer - INFO - Epoch 3, Batch 100/1000 (10.0%), Loss: 3.6646
2025-07-13 10:40:08,792 - KhmerOCRTrainer - INFO - Epoch 3, Batch 200/1000 (20.0%), Loss: 3.6607
2025-07-13 10:43:16,025 - KhmerOCRTrainer - INFO - Epoch 3, Batch 300/1000 (30.0%), Loss: 3.6525
2025-07-13 10:46:30,417 - KhmerOCRTrainer - INFO - Epoch 3, Batch 400/1000 (40.0%), Loss: 3.6724
2025-07-13 10:49:54,579 - KhmerOCRTrainer - INFO - Epoch 3, Batch 500/1000 (50.0%), Loss: 3.6439
2025-07-13 10:53:07,774 - KhmerOCRTrainer - INFO - Epoch 3, Batch 600/1000 (60.0%), Loss: 3.6548
2025-07-13 10:56:16,733 - KhmerOCRTrainer - INFO - Epoch 3, Batch 700/1000 (70.0%), Loss: 3.6577
2025-07-13 10:59:38,444 - KhmerOCRTrainer - INFO - Epoch 3, Batch 800/1000 (80.0%), Loss: 3.6689
2025-07-13 11:02:47,916 - KhmerOCRTrainer - INFO - Epoch 3, Batch 900/1000 (90.0%), Loss: 3.6521
Validation progress: 0.0%
2025-07-13 11:06:18,831 - KhmerOCRTrainer - INFO - 
Epoch 4 Results:
2025-07-13 11:06:18,831 - KhmerOCRTrainer - INFO -   Train Loss: 3.6560
2025-07-13 11:06:18,831 - KhmerOCRTrainer - INFO -   Val Loss: 3.6939
2025-07-13 11:06:18,831 - KhmerOCRTrainer - INFO -   Val CER: 114.79%
2025-07-13 11:06:18,832 - KhmerOCRTrainer - INFO -   Epoch Time: 1946.13s
2025-07-13 11:06:19,295 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_003.pth
2025-07-13 11:06:19,326 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 11:06:19,326 - KhmerOCRTrainer - INFO - Epoch 5/500
2025-07-13 11:06:19,326 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 11:06:27,533 - KhmerOCRTrainer - INFO - Epoch 4, Batch 0/1000 (0.0%), Loss: 3.6484
2025-07-13 11:09:42,584 - KhmerOCRTrainer - INFO - Epoch 4, Batch 100/1000 (10.0%), Loss: 3.6547
2025-07-13 11:12:49,471 - KhmerOCRTrainer - INFO - Epoch 4, Batch 200/1000 (20.0%), Loss: 3.6460
2025-07-13 11:16:00,333 - KhmerOCRTrainer - INFO - Epoch 4, Batch 300/1000 (30.0%), Loss: 3.6566
2025-07-13 11:19:10,520 - KhmerOCRTrainer - INFO - Epoch 4, Batch 400/1000 (40.0%), Loss: 3.6156
2025-07-13 11:22:29,119 - KhmerOCRTrainer - INFO - Epoch 4, Batch 500/1000 (50.0%), Loss: 3.6475
2025-07-13 11:25:34,326 - KhmerOCRTrainer - INFO - Epoch 4, Batch 600/1000 (60.0%), Loss: 3.6330
2025-07-13 11:28:45,282 - KhmerOCRTrainer - INFO - Epoch 4, Batch 700/1000 (70.0%), Loss: 3.6331
2025-07-13 11:31:55,583 - KhmerOCRTrainer - INFO - Epoch 4, Batch 800/1000 (80.0%), Loss: 3.6415
2025-07-13 11:35:14,772 - KhmerOCRTrainer - INFO - Epoch 4, Batch 900/1000 (90.0%), Loss: 3.6168
Validation progress: 0.0%
2025-07-13 11:38:43,705 - KhmerOCRTrainer - INFO - 
Epoch 5 Results:
2025-07-13 11:38:43,705 - KhmerOCRTrainer - INFO -   Train Loss: 3.6404
2025-07-13 11:38:43,705 - KhmerOCRTrainer - INFO -   Val Loss: 3.6948
2025-07-13 11:38:43,705 - KhmerOCRTrainer - INFO -   Val CER: 112.20%
2025-07-13 11:38:43,705 - KhmerOCRTrainer - INFO -   Epoch Time: 1925.73s
2025-07-13 11:38:44,136 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_004.pth
2025-07-13 11:38:44,158 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 11:38:44,159 - KhmerOCRTrainer - INFO - Epoch 6/500
2025-07-13 11:38:44,159 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 11:38:52,986 - KhmerOCRTrainer - INFO - Epoch 5, Batch 0/1000 (0.0%), Loss: 3.6534
2025-07-13 11:42:07,562 - KhmerOCRTrainer - INFO - Epoch 5, Batch 100/1000 (10.0%), Loss: 3.6469
2025-07-13 11:45:16,049 - KhmerOCRTrainer - INFO - Epoch 5, Batch 200/1000 (20.0%), Loss: 3.6213
2025-07-13 11:48:22,886 - KhmerOCRTrainer - INFO - Epoch 5, Batch 300/1000 (30.0%), Loss: 3.6539
2025-07-13 11:51:53,785 - KhmerOCRTrainer - INFO - Epoch 5, Batch 400/1000 (40.0%), Loss: 3.6219
2025-07-13 11:55:18,450 - KhmerOCRTrainer - INFO - Epoch 5, Batch 500/1000 (50.0%), Loss: 3.6180
2025-07-13 11:58:41,476 - KhmerOCRTrainer - INFO - Epoch 5, Batch 600/1000 (60.0%), Loss: 3.6044
2025-07-13 12:01:55,098 - KhmerOCRTrainer - INFO - Epoch 5, Batch 700/1000 (70.0%), Loss: 3.6132
2025-07-13 12:05:25,348 - KhmerOCRTrainer - INFO - Epoch 5, Batch 800/1000 (80.0%), Loss: 3.6239
2025-07-13 12:08:50,659 - KhmerOCRTrainer - INFO - Epoch 5, Batch 900/1000 (90.0%), Loss: 3.6270
Validation progress: 0.0%
2025-07-13 12:12:30,755 - KhmerOCRTrainer - INFO - 
Epoch 6 Results:
2025-07-13 12:12:30,755 - KhmerOCRTrainer - INFO -   Train Loss: 3.6179
2025-07-13 12:12:30,755 - KhmerOCRTrainer - INFO -   Val Loss: 3.6976
2025-07-13 12:12:30,755 - KhmerOCRTrainer - INFO -   Val CER: 109.38%
2025-07-13 12:12:30,755 - KhmerOCRTrainer - INFO -   Epoch Time: 2007.92s
2025-07-13 12:12:30,756 - KhmerOCRTrainer - INFO -   New best CER: 109.38%
2025-07-13 12:12:31,193 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_005.pth
2025-07-13 12:12:31,899 - CheckpointManager - INFO - Saved best model: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/best_model.pth
2025-07-13 12:12:33,008 - CheckpointManager - INFO - Backed up best model to Google Drive: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/best_model.pth
2025-07-13 12:12:33,012 - CheckpointManager - INFO - Saved training history to Google Drive: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/training_history.json
2025-07-13 12:12:33,017 - CheckpointManager - INFO - Removed old checkpoint: checkpoint_epoch_000.pth
2025-07-13 12:12:33,018 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 12:12:33,018 - KhmerOCRTrainer - INFO - Epoch 7/500
2025-07-13 12:12:33,018 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 12:12:42,603 - KhmerOCRTrainer - INFO - Epoch 6, Batch 0/1000 (0.0%), Loss: 3.5815
2025-07-13 12:15:59,806 - KhmerOCRTrainer - INFO - Epoch 6, Batch 100/1000 (10.0%), Loss: 3.6020
2025-07-13 12:19:29,243 - KhmerOCRTrainer - INFO - Epoch 6, Batch 200/1000 (20.0%), Loss: 3.6073
2025-07-13 12:22:48,462 - KhmerOCRTrainer - INFO - Epoch 6, Batch 300/1000 (30.0%), Loss: 3.5883
2025-07-13 12:25:55,207 - KhmerOCRTrainer - INFO - Epoch 6, Batch 400/1000 (40.0%), Loss: 3.5599
2025-07-13 12:29:02,597 - KhmerOCRTrainer - INFO - Epoch 6, Batch 500/1000 (50.0%), Loss: 3.5989
2025-07-13 12:32:09,317 - KhmerOCRTrainer - INFO - Epoch 6, Batch 600/1000 (60.0%), Loss: 3.5849
2025-07-13 12:35:20,074 - KhmerOCRTrainer - INFO - Epoch 6, Batch 700/1000 (70.0%), Loss: 3.5552
2025-07-13 12:38:34,096 - KhmerOCRTrainer - INFO - Epoch 6, Batch 800/1000 (80.0%), Loss: 3.5558
2025-07-13 12:41:44,537 - KhmerOCRTrainer - INFO - Epoch 6, Batch 900/1000 (90.0%), Loss: 3.5532
Validation progress: 0.0%
2025-07-13 12:45:05,414 - KhmerOCRTrainer - INFO - 
Epoch 7 Results:
2025-07-13 12:45:05,415 - KhmerOCRTrainer - INFO -   Train Loss: 3.5777
2025-07-13 12:45:05,415 - KhmerOCRTrainer - INFO -   Val Loss: 3.7730
2025-07-13 12:45:05,415 - KhmerOCRTrainer - INFO -   Val CER: 112.76%
2025-07-13 12:45:05,415 - KhmerOCRTrainer - INFO -   Epoch Time: 1933.73s
2025-07-13 12:45:05,835 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_006.pth
2025-07-13 12:45:05,854 - CheckpointManager - INFO - Removed old checkpoint: checkpoint_epoch_001.pth
2025-07-13 12:45:05,854 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 12:45:05,854 - KhmerOCRTrainer - INFO - Epoch 8/500
2025-07-13 12:45:05,854 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 12:45:17,028 - KhmerOCRTrainer - INFO - Epoch 7, Batch 0/1000 (0.0%), Loss: 3.5422
2025-07-13 12:48:33,541 - KhmerOCRTrainer - INFO - Epoch 7, Batch 100/1000 (10.0%), Loss: 3.5581
2025-07-13 12:51:43,397 - KhmerOCRTrainer - INFO - Epoch 7, Batch 200/1000 (20.0%), Loss: 3.5448
2025-07-13 12:54:59,321 - KhmerOCRTrainer - INFO - Epoch 7, Batch 300/1000 (30.0%), Loss: 3.5226
2025-07-13 12:58:12,788 - KhmerOCRTrainer - INFO - Epoch 7, Batch 400/1000 (40.0%), Loss: 3.5364
2025-07-13 13:01:19,876 - KhmerOCRTrainer - INFO - Epoch 7, Batch 500/1000 (50.0%), Loss: 3.5281
2025-07-13 13:04:23,703 - KhmerOCRTrainer - INFO - Epoch 7, Batch 600/1000 (60.0%), Loss: 3.5415
2025-07-13 13:07:33,908 - KhmerOCRTrainer - INFO - Epoch 7, Batch 700/1000 (70.0%), Loss: 3.5315
2025-07-13 13:10:50,557 - KhmerOCRTrainer - INFO - Epoch 7, Batch 800/1000 (80.0%), Loss: 3.5214
2025-07-13 13:14:01,794 - KhmerOCRTrainer - INFO - Epoch 7, Batch 900/1000 (90.0%), Loss: 3.5206
Validation progress: 0.0%
2025-07-13 13:17:33,534 - KhmerOCRTrainer - INFO - 
Epoch 8 Results:
2025-07-13 13:17:33,534 - KhmerOCRTrainer - INFO -   Train Loss: 3.5273
2025-07-13 13:17:33,535 - KhmerOCRTrainer - INFO -   Val Loss: 3.8111
2025-07-13 13:17:33,535 - KhmerOCRTrainer - INFO -   Val CER: 112.91%
2025-07-13 13:17:33,535 - KhmerOCRTrainer - INFO -   Epoch Time: 1929.03s
2025-07-13 13:17:33,992 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_007.pth
2025-07-13 13:17:34,014 - CheckpointManager - INFO - Removed old checkpoint: checkpoint_epoch_002.pth
2025-07-13 13:17:34,014 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 13:17:34,014 - KhmerOCRTrainer - INFO - Epoch 9/500
2025-07-13 13:17:34,014 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 13:17:44,311 - KhmerOCRTrainer - INFO - Epoch 8, Batch 0/1000 (0.0%), Loss: 3.4994
2025-07-13 13:21:00,134 - KhmerOCRTrainer - INFO - Epoch 8, Batch 100/1000 (10.0%), Loss: 3.5003
2025-07-13 13:24:07,402 - KhmerOCRTrainer - INFO - Epoch 8, Batch 200/1000 (20.0%), Loss: 3.4947
2025-07-13 13:27:34,102 - KhmerOCRTrainer - INFO - Epoch 8, Batch 300/1000 (30.0%), Loss: 3.4822
2025-07-13 13:30:55,268 - KhmerOCRTrainer - INFO - Epoch 8, Batch 400/1000 (40.0%), Loss: 3.4793
2025-07-13 13:34:13,395 - KhmerOCRTrainer - INFO - Epoch 8, Batch 500/1000 (50.0%), Loss: 3.4851
2025-07-13 13:37:25,708 - KhmerOCRTrainer - INFO - Epoch 8, Batch 600/1000 (60.0%), Loss: 3.4895
2025-07-13 13:40:38,068 - KhmerOCRTrainer - INFO - Epoch 8, Batch 700/1000 (70.0%), Loss: 3.4782
2025-07-13 13:43:57,399 - KhmerOCRTrainer - INFO - Epoch 8, Batch 800/1000 (80.0%), Loss: 3.4820
2025-07-13 13:47:11,987 - KhmerOCRTrainer - INFO - Epoch 8, Batch 900/1000 (90.0%), Loss: 3.4762
Validation progress: 0.0%
2025-07-13 13:50:39,335 - KhmerOCRTrainer - INFO - 
Epoch 9 Results:
2025-07-13 13:50:39,336 - KhmerOCRTrainer - INFO -   Train Loss: 3.4908
2025-07-13 13:50:39,336 - KhmerOCRTrainer - INFO -   Val Loss: 3.8276
2025-07-13 13:50:39,336 - KhmerOCRTrainer - INFO -   Val CER: 112.97%
2025-07-13 13:50:39,336 - KhmerOCRTrainer - INFO -   Epoch Time: 1966.64s
2025-07-13 13:50:39,775 - CheckpointManager - INFO - Saved checkpoint: /content/drive/MyDrive/KhmerOCR_Checkpoints/models/checkpoint_epoch_008.pth
2025-07-13 13:50:39,779 - CheckpointManager - INFO - Removed old checkpoint: checkpoint_epoch_003.pth
2025-07-13 13:50:39,779 - KhmerOCRTrainer - INFO - 
============================================================
2025-07-13 13:50:39,779 - KhmerOCRTrainer - INFO - Epoch 10/500
2025-07-13 13:50:39,779 - KhmerOCRTrainer - INFO - ============================================================
2025-07-13 13:50:47,932 - KhmerOCRTrainer - INFO - Epoch 9, Batch 0/1000 (0.0%), Loss: 3.4700
2025-07-13 13:50:53,191 - KhmerOCROnTheFlyTraining - INFO - 
Training interrupted by user
âœ… Training completed!




===============================

Looking at your training logs, I can see the progression across two training sessions. Let me analyze the training performance:

## Training Progress Summary

### **Session 1** (`traing2_e0-e1_b64x2_s100000.txt`)
- **Configuration**: Batch size 64, 100K samples/epoch
- **Epochs**: 1-2 (interrupted at epoch 3)
- **Initial Performance**: Started with 4.7060 loss, 114.43% CER
- **Final Performance**: 3.6845 loss, 111.90% CER
- **Progress**: âœ… Good initial learning, CER improved by ~2.5%

### **Session 2** (`traing2_e2-e_b128x2_s128000.txt`)
- **Configuration**: Batch size 128, 128K samples/epoch (scaled up)
- **Epochs**: 3-8+ (resumed from previous session)
- **Performance Trend**:

| Epoch | Train Loss | Val Loss | Val CER | Status |
|-------|------------|----------|---------|--------|
| 3     | 3.6688     | 3.6932   | 111.80% | âœ… Slight improvement |
| 4     | 3.6560     | 3.6939   | 114.79% | âš ï¸ CER degraded |
| 5     | 3.6404     | 3.6948   | 112.20% | â†—ï¸ Recovering |
| 6     | 3.6179     | 3.6976   | 109.38% | âœ… **Best CER so far** |
| 7     | 3.5777     | 3.7730   | 112.76% | âš ï¸ Validation diverging |
| 8     | 3.5273     | 3.8111   | 112.91% | âš ï¸ Overfitting signs |

## Key Observations

### âœ… **Positive Trends**
- **Training loss consistently decreasing**: 4.7060 â†’ 3.5273 (25% improvement)
- **Best CER achieved**: 109.38% at epoch 6
- **Model learning**: Clear progression from initial random state

### âš ï¸ **Concerns**
- **Validation loss increasing**: 3.6932 â†’ 3.8111 (potential overfitting)
- **CER instability**: Fluctuating between 109-115% after epoch 6
- **Train-validation gap**: Growing divergence suggests overfitting

### ðŸŽ¯ **Current Status**
- **Target CER**: â‰¤1.0% (you're at 109.38% - still far from target)
- **Training stage**: Early stages, model still learning basic patterns
- **Recommendation**: Continue training but monitor for overfitting

## Recommendations

1. **Continue training** - You're still in early stages for OCR
2. **Monitor validation closely** - Consider early stopping if overfitting worsens
3. **Possible adjustments**:
   - Reduce learning rate if validation loss keeps increasing
   - Add regularization (dropout, weight decay)
   - Increase validation frequency

The training is progressing reasonably for a complex OCR task, but you'll likely need many more epochs to reach the target 1% CER.


=====================================
Looking at your training logs, **yes, I recommend changing back to batch size 64**. Here's why:

## Analysis of Batch Size Impact

### **Batch Size 64 Performance** (Session 1)
- âœ… **Stable learning**: Consistent improvement from 114.43% â†’ 111.90% CER
- âœ… **Good train-val alignment**: Validation loss stayed close to training loss
- âœ… **Smooth convergence**: No signs of overfitting in the short run

### **Batch Size 128 Performance** (Session 2) 
- âš ï¸ **Validation divergence**: Val loss increased from 3.6932 â†’ 3.8111
- âš ï¸ **CER instability**: Fluctuating performance after epoch 6
- âš ï¸ **Overfitting signs**: Growing gap between train and validation loss

## Why Batch Size 64 is Better Here

### **1. Better Generalization**
- Smaller batches provide noisier gradients â†’ better exploration
- Helps escape local minima that larger batches might get stuck in
- OCR tasks often benefit from more frequent parameter updates

### **2. Memory Efficiency**
- Allows for more complex augmentation without OOM errors
- Better for the on-the-fly image generation system

### **3. Learning Rate Compatibility**
- Your learning rate (1e-06) was tuned for smaller batches
- Larger batches typically need higher learning rates (linear scaling rule)

## Recommended Changes

```yaml
# configs/train_config.yaml
batch_size: 64                    # Change from 128 â†’ 64
samples_per_epoch: 100000        # Change from 128000 â†’ 100000
learning_rate: 1e-06             # Keep same (was working well)
```

## Expected Benefits

1. **More stable validation loss**
2. **Better CER consistency** 
3. **Improved generalization**
4. **Faster feedback loop** (shorter epochs)

## Alternative: Gradual Scaling
If you want to keep larger batches, try:
- Increase learning rate to 1.6e-06 (128/64 Ã— 1e-06)
- Add more regularization (dropout, weight decay)

But given your current overfitting trend, **batch size 64 is the safer choice** to get back on track.


====================================================



**Batch size 32 could be even better!** Here's my analysis:

## Batch Size 32 Analysis

### âœ… **Potential Benefits**

**1. Superior Exploration**
- **Noisier gradients** = better escape from local minima
- **More frequent updates** = 3,125 batches/epoch vs 1,563 (64) or 1,000 (128)
- **Better for complex landscapes** like OCR character recognition

**2. OCR-Specific Advantages**
- **High variance in text complexity** â†’ smaller batches handle this better
- **On-the-fly generation** works excellently with smaller batches
- **Character-level learning** often benefits from more frequent updates

**3. Memory & Augmentation**
- **Lower memory pressure** = more aggressive augmentation possible
- **Better cache utilization** for synthetic image generation

### âš ï¸ **Potential Drawbacks**

**1. Training Time**
- **Longer wall-clock time**: ~2x iterations per epoch
- **More verbose logging**: 3,125 batches vs 1,563

**2. Stability Concerns**
- **Noisier training curves** (but this can be good for exploration)
- **May need learning rate adjustment**

## Recommended Configuration

```yaml
# configs/train_config.yaml
batch_size: 32                    # Aggressive but potentially optimal
samples_per_epoch: 100000        # Keep same total samples
learning_rate: 1e-06             # Start with same, monitor closely
```

## Expected Performance

| Metric | Batch 128 | Batch 64 | **Batch 32** |
|--------|-----------|----------|-------------|
| Convergence | Unstable | Good | **Potentially better** |
| Generalization | Poor | Good | **Excellent** |
| Training time | Fast | Medium | Slower |
| Memory usage | High | Medium | **Low** |

## Decision Framework

**Choose batch size 32 if:**
- âœ… You can tolerate longer training time
- âœ… You want maximum exploration capability
- âœ… You're seeing overfitting with larger batches

**Stick with batch size 64 if:**
- âš ï¸ You need faster iteration cycles
- âš ï¸ You want proven stable performance

## My Recommendation

**Try batch size 32** - OCR tasks often benefit from smaller batches, and given your overfitting issues with 128, going smaller is the right direction. The extra training time is worth it for better convergence.

**Monitor these metrics:**
- Validation loss should stay closer to training loss
- CER should improve more consistently
- Training curves should be noisier but trending downward


=================================

Thank you for the update. The results from epoch 9 are very telling and confirm our suspicions.

## Updated Training Performance

The trend of overfitting is now undeniable. While your training loss continues to improve, the validation loss and CER are getting worse.

| Epoch | Train Loss | Val Loss | Val CER | Status |
|-------|------------|----------|---------|--------|
| ...   | ...        | ...      | ...     | ...    |
| 6     | 3.6179     | 3.6976   | 109.38% | âœ… **Best CER so far** |
| 7     | 3.5777     | 3.7730   | 112.76% | âš ï¸ Validation diverging |
| 8     | 3.5273     | 3.8111   | 112.91% | âš ï¸ Overfitting signs |
| **9** | **3.4908** | **3.8276** | **112.97%** | ðŸš¨ **Overfitting Confirmed** |

<br>

## Analysis of Epoch 9

- **Train Loss**: Decreased by 1.0% (Good)
- **Validation Loss**: Increased by 0.4% (Bad)
- **Validation CER**: Increased slightly (Bad)

This shows your model is getting better at memorizing the training data but is failing to generalize to new, unseen data. The gap between training performance and validation performance is widening, which is the classic definition of overfitting.

## Strong Recommendation: Reduce Batch Size Now

Continuing with a batch size of 128 will likely just waste time and resources as the model continues to overfit.

My previous advice to **reduce the batch size to 32 (recommended) or 64 (safer)** is now more critical than ever. A smaller batch size acts as a regularizer, introducing noise into the learning process that forces the model to learn more robust, general features instead of memorizing the training set.

### Action to Take

1.  **Stop any current training session.**
2.  **Edit your configuration file.** I'll find `train_config.yaml` and apply the change to use a batch size of 32.
3.  **Restart the training script.** The trainer should automatically resume from the last checkpoint (`checkpoint_epoch_008.pth`) and apply the new batch size for epoch 10 and beyond.

I will now locate `configs/train_config.yaml` and change the batch size to 32 for you.