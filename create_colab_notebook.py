#!/usr/bin/env python3
"""
Script to generate a comprehensive Jupyter notebook for Khmer OCR training on Google Colab
"""

import json
import os

def create_colab_notebook():
    """Create a complete Jupyter notebook for Google Colab training"""
    
    notebook = {
        "cells": [
            # Header cell
            {
                "cell_type": "markdown",
                "metadata": {"id": "khmer_ocr_header"},
                "source": [
                    "# ðŸ‡°ðŸ‡­ Khmer OCR Seq2Seq Training on Google Colab\n",
                    "\n",
                    "**Attention-Based Khmer OCR Engine with On-the-Fly Training**\n",
                    "\n",
                    "This notebook implements the complete training pipeline for the Khmer OCR system using:\n",
                    "- **On-the-fly image generation** for efficient training\n",
                    "- **Fixed validation set** (6,400 images) for reproducible evaluation\n",
                    "- **Resumable training** with Google Drive checkpoint saving\n",
                    "- **GPU optimization** for Google Colab Pro/Pro+\n",
                    "\n",
                    "**Target Performance:** â‰¤ 1.0% Character Error Rate (CER)\n",
                    "\n",
                    "---\n",
                    "\n",
                    "## ðŸ“‹ Prerequisites\n",
                    "\n",
                    "**Required:**\n",
                    "- Google Colab Pro/Pro+ (recommended for GPU memory)\n",
                    "- Google Drive account (for checkpoint storage)\n",
                    "- GitHub access to clone the repository\n",
                    "\n",
                    "**Recommended Runtime:**\n",
                    "- GPU: Tesla V100, A100, or T4 (minimum 16GB VRAM)\n",
                    "- RAM: High-RAM runtime (25GB+)\n",
                    "- Storage: 50GB+ Google Drive space for checkpoints\n",
                    "\n",
                    "---"
                ]
            },
            
            # System check cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "check_runtime"},
                "outputs": [],
                "source": [
                    "# Check GPU availability and specs\n",
                    "import torch\n",
                    "import subprocess\n",
                    "import psutil\n",
                    "import os\n",
                    "\n",
                    "print(\"ðŸ”§ System Information:\")\n",
                    "print(f\"   Python version: {torch.version}\")\n",
                    "print(f\"   PyTorch version: {torch.__version__}\")\n",
                    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
                    "\n",
                    "if torch.cuda.is_available():\n",
                    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
                    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
                    "else:\n",
                    "    print(\"   âš ï¸  No GPU available - switch to GPU runtime!\")\n",
                    "\n",
                    "print(f\"   RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
                    "print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
                    "\n",
                    "# Check available disk space\n",
                    "disk_usage = psutil.disk_usage('/')\n",
                    "print(f\"   Disk space: {disk_usage.free / 1024**3:.1f} GB free / {disk_usage.total / 1024**3:.1f} GB total\")"
                ]
            },
            
            # Google Drive mount cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "mount_google_drive"},
                "outputs": [],
                "source": [
                    "# Mount Google Drive for checkpoint storage\n",
                    "from google.colab import drive\n",
                    "import os\n",
                    "\n",
                    "print(\"ðŸ“ Mounting Google Drive...\")\n",
                    "drive.mount('/content/drive')\n",
                    "\n",
                    "# Create checkpoint directory structure\n",
                    "checkpoint_base = '/content/drive/MyDrive/KhmerOCR_Checkpoints'\n",
                    "os.makedirs(checkpoint_base, exist_ok=True)\n",
                    "os.makedirs(f'{checkpoint_base}/models', exist_ok=True)\n",
                    "os.makedirs(f'{checkpoint_base}/logs', exist_ok=True)\n",
                    "os.makedirs(f'{checkpoint_base}/validation_set', exist_ok=True)\n",
                    "\n",
                    "print(f\"âœ… Google Drive mounted at: {checkpoint_base}\")\n",
                    "\n",
                    "# List existing checkpoints\n",
                    "model_dir = f'{checkpoint_base}/models'\n",
                    "if os.path.exists(model_dir):\n",
                    "    checkpoints = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
                    "    if checkpoints:\n",
                    "        print(f\"ðŸ“¦ Found {len(checkpoints)} existing checkpoints:\")\n",
                    "        for cp in sorted(checkpoints):\n",
                    "            print(f\"   - {cp}\")\n",
                    "    else:\n",
                    "        print(\"ðŸ“¦ No existing checkpoints found\")"
                ]
            },
            
            # Clone repository cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "clone_repository"},
                "outputs": [],
                "source": [
                    "# Clone the Khmer OCR repository\n",
                    "import subprocess\n",
                    "import os\n",
                    "\n",
                    "repo_url = \"https://github.com/kunthet/khmer-ocr-seq2seq.git\"\n",
                    "repo_dir = \"/content/khmer-ocr-seq2seq\"\n",
                    "\n",
                    "# Remove existing directory if it exists\n",
                    "if os.path.exists(repo_dir):\n",
                    "    print(\"ðŸ—‘ï¸  Removing existing repository...\")\n",
                    "    subprocess.run([\"rm\", \"-rf\", repo_dir], check=True)\n",
                    "\n",
                    "# Clone the repository\n",
                    "print(f\"ðŸ“¥ Cloning repository from {repo_url}...\")\n",
                    "result = subprocess.run([\"git\", \"clone\", repo_url, repo_dir], \n",
                    "                       capture_output=True, text=True)\n",
                    "\n",
                    "if result.returncode == 0:\n",
                    "    print(\"âœ… Repository cloned successfully!\")\n",
                    "    \n",
                    "    # Change to repository directory\n",
                    "    os.chdir(repo_dir)\n",
                    "    print(f\"ðŸ“‚ Changed to directory: {os.getcwd()}\")\n",
                    "    \n",
                    "    # Show recent commits\n",
                    "    commits = subprocess.run([\"git\", \"log\", \"--oneline\", \"-5\"], \n",
                    "                           capture_output=True, text=True)\n",
                    "    print(\"\\nðŸ“œ Recent commits:\")\n",
                    "    print(commits.stdout)\n",
                    "else:\n",
                    "    print(f\"âŒ Failed to clone repository: {result.stderr}\")\n",
                    "    raise Exception(\"Repository clone failed\")"
                ]
            },
            
            # Install dependencies cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "install_dependencies"},
                "outputs": [],
                "source": [
                    "# Install Python dependencies\n",
                    "print(\"ðŸ“¦ Installing Python dependencies...\")\n",
                    "\n",
                    "# Install packages from requirements.txt if it exists\n",
                    "if os.path.exists('requirements.txt'):\n",
                    "    !pip install -r requirements.txt\n",
                    "else:\n",
                    "    # Install essential packages\n",
                    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                    "    !pip install pillow numpy matplotlib tqdm\n",
                    "    !pip install tensorboard wandb\n",
                    "    !pip install opencv-python-headless\n",
                    "    !pip install scikit-learn\n",
                    "    !pip install Pillow-SIMD  # For better image processing performance\n",
                    "\n",
                    "# Install additional Colab-specific packages\n",
                    "!pip install ipywidgets\n",
                    "!pip install plotly\n",
                    "\n",
                    "print(\"âœ… Dependencies installed successfully!\")\n",
                    "\n",
                    "# Verify installation\n",
                    "import torch\n",
                    "import torchvision\n",
                    "import PIL\n",
                    "import cv2\n",
                    "import numpy as np\n",
                    "import matplotlib.pyplot as plt\n",
                    "\n",
                    "print(f\"\\nðŸ” Package versions:\")\n",
                    "print(f\"   PyTorch: {torch.__version__}\")\n",
                    "print(f\"   Torchvision: {torchvision.__version__}\")\n",
                    "print(f\"   Pillow: {PIL.__version__}\")\n",
                    "print(f\"   OpenCV: {cv2.__version__}\")\n",
                    "print(f\"   NumPy: {np.__version__}\")"
                ]
            },
            
            # Generate validation set cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "generate_validation_set"},
                "outputs": [],
                "source": [
                    "# Generate or load fixed validation set\n",
                    "validation_local = \"data/validation_fixed\"\n",
                    "validation_drive = f\"{checkpoint_base}/validation_set\"\n",
                    "\n",
                    "print(\"ðŸ” Checking validation set...\")\n",
                    "\n",
                    "# Check if validation set exists on Google Drive\n",
                    "if os.path.exists(f\"{validation_drive}/metadata.json\"):\n",
                    "    print(\"âœ… Validation set found on Google Drive\")\n",
                    "    \n",
                    "    # Copy from Google Drive to local\n",
                    "    print(\"ðŸ“¥ Copying validation set from Google Drive...\")\n",
                    "    !cp -r \"{validation_drive}\" \"{validation_local}\"\n",
                    "    \n",
                    "    # Verify local copy\n",
                    "    if os.path.exists(f\"{validation_local}/metadata.json\"):\n",
                    "        print(\"âœ… Validation set copied successfully!\")\n",
                    "    else:\n",
                    "        print(\"âŒ Failed to copy validation set\")\n",
                    "        \n",
                    "elif os.path.exists(f\"{validation_local}/metadata.json\"):\n",
                    "    print(\"âœ… Validation set found locally\")\n",
                    "    \n",
                    "    # Copy to Google Drive for backup\n",
                    "    print(\"ðŸ“¤ Backing up validation set to Google Drive...\")\n",
                    "    !cp -r \"{validation_local}\" \"{validation_drive}\"\n",
                    "    print(\"âœ… Validation set backed up to Google Drive\")\n",
                    "    \n",
                    "else:\n",
                    "    print(\"âŒ No validation set found. Generating new one...\")\n",
                    "    \n",
                    "    # Generate validation set\n",
                    "    corpus_dir = \"data/processed\"\n",
                    "    fonts_dir = \"fonts\"\n",
                    "    \n",
                    "    if os.path.exists(corpus_dir) and os.path.exists(fonts_dir):\n",
                    "        print(\"ðŸ”„ Generating 6,400 validation images...\")\n",
                    "        \n",
                    "        # Run validation set generation\n",
                    "        !python generate_fixed_validation_set.py --num-samples 6400 --output-dir \"{validation_local}\" --corpus-dir \"{corpus_dir}\" --fonts-dir \"{fonts_dir}\"\n",
                    "        \n",
                    "        # Check if generation was successful\n",
                    "        if os.path.exists(f\"{validation_local}/metadata.json\"):\n",
                    "            print(\"âœ… Validation set generated successfully!\")\n",
                    "            \n",
                    "            # Backup to Google Drive\n",
                    "            print(\"ðŸ“¤ Backing up to Google Drive...\")\n",
                    "            !cp -r \"{validation_local}\" \"{validation_drive}\"\n",
                    "            print(\"âœ… Validation set backed up to Google Drive\")\n",
                    "        else:\n",
                    "            print(\"âŒ Failed to generate validation set\")\n",
                    "    else:\n",
                    "        print(\"âŒ Cannot generate validation set - missing corpus or fonts\")\n",
                    "\n",
                    "# Verify validation set\n",
                    "if os.path.exists(f\"{validation_local}/metadata.json\"):\n",
                    "    import json\n",
                    "    with open(f\"{validation_local}/metadata.json\", 'r') as f:\n",
                    "        metadata = json.load(f)\n",
                    "    \n",
                    "    print(f\"\\nðŸ“Š Validation Set Statistics:\")\n",
                    "    print(f\"   Total samples: {metadata['total_samples']:,}\")\n",
                    "    print(f\"   Fonts used: {len(metadata['fonts_used'])}\")\n",
                    "    print(f\"   Image height: {metadata['generation_config']['image_height']}px\")\n",
                    "    print(f\"   Variable width: {metadata['generation_config']['variable_width']}\")\n",
                    "    print(f\"   Random seed: {metadata['generation_config']['random_seed']}\")"
                ]
            },
            
            # Training configuration cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "training_config"},
                "outputs": [],
                "source": [
                    "# Training configuration\n",
                    "TRAINING_CONFIG = {\n",
                    "    # Model parameters\n",
                    "    \"vocab_size\": 117,  # As per PRD\n",
                    "    \"embedding_dim\": 256,\n",
                    "    \"encoder_hidden_size\": 256,\n",
                    "    \"decoder_hidden_size\": 512,\n",
                    "    \"attention_dim\": 256,\n",
                    "    \"image_height\": 32,\n",
                    "    \"image_channels\": 1,\n",
                    "    \n",
                    "    # Training parameters\n",
                    "    \"batch_size\": 32,  # Will be adjusted based on GPU memory\n",
                    "    \"learning_rate\": 1e-6,  # As per PRD\n",
                    "    \"num_epochs\": 150,  # As per PRD\n",
                    "    \"teacher_forcing_ratio\": 1.0,  # As per PRD\n",
                    "    \n",
                    "    # On-the-fly generation parameters\n",
                    "    \"samples_per_epoch\": 10000,  # Number of samples to generate per epoch\n",
                    "    \"max_sequence_length\": 256,\n",
                    "    \n",
                    "    # Checkpoint and logging\n",
                    "    \"save_every_n_epochs\": 5,\n",
                    "    \"validate_every_n_epochs\": 1,\n",
                    "    \"log_every_n_batches\": 100,\n",
                    "    \n",
                    "    # Paths\n",
                    "    \"corpus_dir\": \"data/processed\",\n",
                    "    \"fonts_dir\": \"fonts\",\n",
                    "    \"validation_dir\": \"data/validation_fixed\",\n",
                    "    \"checkpoint_dir\": f\"{checkpoint_base}/models\",\n",
                    "    \"log_dir\": f\"{checkpoint_base}/logs\",\n",
                    "    \n",
                    "    # Performance\n",
                    "    \"num_workers\": 2,  # For DataLoader\n",
                    "    \"pin_memory\": True,\n",
                    "    \"mixed_precision\": True,  # Use automatic mixed precision\n",
                    "}\n",
                    "\n",
                    "# Auto-adjust batch size based on GPU memory\n",
                    "if torch.cuda.is_available():\n",
                    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                    "    \n",
                    "    if gpu_memory_gb >= 40:  # A100 40GB\n",
                    "        TRAINING_CONFIG[\"batch_size\"] = 64\n",
                    "    elif gpu_memory_gb >= 24:  # RTX 3090, RTX 4090\n",
                    "        TRAINING_CONFIG[\"batch_size\"] = 48\n",
                    "    elif gpu_memory_gb >= 16:  # V100, P100\n",
                    "        TRAINING_CONFIG[\"batch_size\"] = 32\n",
                    "    elif gpu_memory_gb >= 12:  # RTX 3060, RTX 4060\n",
                    "        TRAINING_CONFIG[\"batch_size\"] = 24\n",
                    "    else:  # T4, K80\n",
                    "        TRAINING_CONFIG[\"batch_size\"] = 16\n",
                    "    \n",
                    "    print(f\"ðŸŽ¯ Auto-adjusted batch size to {TRAINING_CONFIG['batch_size']} for {gpu_memory_gb:.1f}GB GPU\")\n",
                    "\n",
                    "# Print configuration\n",
                    "print(\"\\nâš™ï¸  Training Configuration:\")\n",
                    "for key, value in TRAINING_CONFIG.items():\n",
                    "    if not key.endswith('_dir'):\n",
                    "        print(f\"   {key}: {value}\")\n",
                    "\n",
                    "print(\"\\nðŸ“ Paths:\")\n",
                    "for key, value in TRAINING_CONFIG.items():\n",
                    "    if key.endswith('_dir'):\n",
                    "        print(f\"   {key}: {value}\")"
                ]
            },
            
            # Main training cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "main_training"},
                "outputs": [],
                "source": [
                    "# Run the on-the-fly training\n",
                    "print(\"ðŸš€ Starting Khmer OCR Training...\")\n",
                    "\n",
                    "# Use the new on-the-fly training script\n",
                    "!python src/training/train_onthefly.py \\\n",
                    "    --corpus-dir \"{TRAINING_CONFIG['corpus_dir']}\" \\\n",
                    "    --fonts-dir \"{TRAINING_CONFIG['fonts_dir']}\" \\\n",
                    "    --validation-dir \"{TRAINING_CONFIG['validation_dir']}\" \\\n",
                    "    --checkpoint-dir \"{TRAINING_CONFIG['checkpoint_dir']}\" \\\n",
                    "    --batch-size {TRAINING_CONFIG['batch_size']} \\\n",
                    "    --learning-rate {TRAINING_CONFIG['learning_rate']} \\\n",
                    "    --num-epochs {TRAINING_CONFIG['num_epochs']} \\\n",
                    "    --samples-per-epoch {TRAINING_CONFIG['samples_per_epoch']} \\\n",
                    "    --save-every {TRAINING_CONFIG['save_every_n_epochs']} \\\n",
                    "    --validate-every {TRAINING_CONFIG['validate_every_n_epochs']} \\\n",
                    "    --mixed-precision \\\n",
                    "    --resume\n",
                    "\n",
                    "print(\"âœ… Training completed!\")"
                ]
            },
            
            # Results visualization cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "visualize_results"},
                "outputs": [],
                "source": [
                    "# Visualize training results\n",
                    "import matplotlib.pyplot as plt\n",
                    "import json\n",
                    "import os\n",
                    "\n",
                    "# Load training history\n",
                    "history_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'training_history.json')\n",
                    "\n",
                    "if os.path.exists(history_path):\n",
                    "    with open(history_path, 'r') as f:\n",
                    "        training_history = json.load(f)\n",
                    "    \n",
                    "    # Extract data for plotting\n",
                    "    epochs = [h['epoch'] for h in training_history]\n",
                    "    train_losses = [h['train_loss'] for h in training_history]\n",
                    "    val_losses = [h['val_loss'] for h in training_history if h['val_loss'] > 0]\n",
                    "    val_cers = [h['val_cer'] for h in training_history if h['val_cer'] > 0]\n",
                    "    \n",
                    "    val_epochs = [h['epoch'] for h in training_history if h['val_cer'] > 0]\n",
                    "    \n",
                    "    # Create subplots\n",
                    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                    "    fig.suptitle('Khmer OCR Training Results', fontsize=16)\n",
                    "    \n",
                    "    # Training loss\n",
                    "    axes[0, 0].plot(epochs, train_losses, 'b-', label='Training Loss')\n",
                    "    if val_losses:\n",
                    "        axes[0, 0].plot(val_epochs, val_losses, 'r-', label='Validation Loss')\n",
                    "    axes[0, 0].set_title('Training & Validation Loss')\n",
                    "    axes[0, 0].set_xlabel('Epoch')\n",
                    "    axes[0, 0].set_ylabel('Loss')\n",
                    "    axes[0, 0].legend()\n",
                    "    axes[0, 0].grid(True)\n",
                    "    \n",
                    "    # Character Error Rate\n",
                    "    if val_cers:\n",
                    "        axes[0, 1].plot(val_epochs, val_cers, 'g-', label='CER', linewidth=2)\n",
                    "        axes[0, 1].axhline(y=0.01, color='r', linestyle='--', label='Target 1% CER')\n",
                    "        axes[0, 1].set_title('Character Error Rate (CER)')\n",
                    "        axes[0, 1].set_xlabel('Epoch')\n",
                    "        axes[0, 1].set_ylabel('CER')\n",
                    "        axes[0, 1].legend()\n",
                    "        axes[0, 1].grid(True)\n",
                    "    \n",
                    "    # Learning curve (CER vs Epochs)\n",
                    "    if val_cers:\n",
                    "        axes[1, 0].semilogy(val_epochs, val_cers, 'orange', label='CER (log scale)')\n",
                    "        axes[1, 0].axhline(y=0.01, color='r', linestyle='--', label='Target 1%')\n",
                    "        axes[1, 0].set_title('Learning Curve (CER Log Scale)')\n",
                    "        axes[1, 0].set_xlabel('Epoch')\n",
                    "        axes[1, 0].set_ylabel('CER (log scale)')\n",
                    "        axes[1, 0].legend()\n",
                    "        axes[1, 0].grid(True)\n",
                    "    \n",
                    "    # Performance summary\n",
                    "    axes[1, 1].axis('off')\n",
                    "    if val_cers:\n",
                    "        best_cer = min(val_cers)\n",
                    "        final_cer = val_cers[-1]\n",
                    "        target_achieved = best_cer <= 0.01\n",
                    "        \n",
                    "        summary_text = f\"\"\"\n",
                    "        Training Summary:\n",
                    "        \n",
                    "        Total Epochs: {len(epochs)}\n",
                    "        Final Training Loss: {train_losses[-1]:.4f}\n",
                    "        Final Validation CER: {final_cer:.4f}\n",
                    "        Best Validation CER: {best_cer:.4f}\n",
                    "        \n",
                    "        Target Achievement:\n",
                    "        Target CER â‰¤ 1%: {'âœ… ACHIEVED' if target_achieved else 'âŒ Not achieved'}\n",
                    "        \n",
                    "        Model Performance:\n",
                    "        {'ðŸ† Excellent!' if best_cer <= 0.005 else 'ðŸŽ¯ Good!' if best_cer <= 0.01 else 'ðŸ“ˆ Needs improvement'}\n",
                    "        \"\"\"\n",
                    "        \n",
                    "        axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, \n",
                    "                        fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
                    "    \n",
                    "    plt.tight_layout()\n",
                    "    plt.show()\n",
                    "    \n",
                    "    # Save plot to Google Drive\n",
                    "    plot_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'training_results.png')\n",
                    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
                    "    print(f\"ðŸ“Š Training results plot saved: {plot_path}\")\n",
                    "    \n",
                    "else:\n",
                    "    print(\"âŒ No training history found\")"
                ]
            },
            
            # Final summary cell
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "final_summary"},
                "outputs": [],
                "source": [
                    "# Final training summary\n",
                    "print(\"ðŸŽ¯ KHMER OCR TRAINING SUMMARY\")\n",
                    "print(\"=\"*50)\n",
                    "\n",
                    "# Load and display final results\n",
                    "best_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'best_model.pth')\n",
                    "latest_checkpoint = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'latest_checkpoint.pth')\n",
                    "\n",
                    "if os.path.exists(best_model_path):\n",
                    "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
                    "    \n",
                    "    print(f\"ðŸ“Š Best Model Performance:\")\n",
                    "    print(f\"   Epoch: {checkpoint['epoch']}\")\n",
                    "    print(f\"   Validation CER: {checkpoint['val_cer']:.4f}\")\n",
                    "    print(f\"   Training Loss: {checkpoint['train_loss']:.4f}\")\n",
                    "    print(f\"   Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
                    "    \n",
                    "    # Target achievement\n",
                    "    target_achieved = checkpoint['val_cer'] <= 0.01\n",
                    "    print(f\"\\nðŸŽ¯ Target Achievement:\")\n",
                    "    print(f\"   Target CER â‰¤ 1%: {'âœ… ACHIEVED' if target_achieved else 'âŒ Not achieved'}\")\n",
                    "    \n",
                    "    if target_achieved:\n",
                    "        print(f\"   ðŸ† Congratulations! Your model achieved the target performance!\")\n",
                    "    else:\n",
                    "        print(f\"   ðŸ“ˆ Consider continuing training or adjusting hyperparameters\")\n",
                    "        \n",
                    "elif os.path.exists(latest_checkpoint):\n",
                    "    checkpoint = torch.load(latest_checkpoint, map_location='cpu')\n",
                    "    print(f\"ðŸ“Š Latest Model Performance:\")\n",
                    "    print(f\"   Epoch: {checkpoint['epoch']}\")\n",
                    "    print(f\"   Validation CER: {checkpoint['val_cer']:.4f}\")\n",
                    "    print(f\"   Training Loss: {checkpoint['train_loss']:.4f}\")\n",
                    "    \n",
                    "else:\n",
                    "    print(\"âŒ No model checkpoints found\")\n",
                    "\n",
                    "print(f\"\\nðŸ’¾ Checkpoints Location:\")\n",
                    "print(f\"   Google Drive: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
                    "print(f\"   Local: /content/khmer-ocr-seq2seq/\")\n",
                    "\n",
                    "print(f\"\\nðŸ”— Resources:\")\n",
                    "print(f\"   Repository: https://github.com/kunthet/khmer-ocr-seq2seq.git\")\n",
                    "print(f\"   Documentation: docs/ON_THE_FLY_TRAINING.md\")\n",
                    "print(f\"   PRD: docs/prd.md\")\n",
                    "\n",
                    "print(\"\\n\" + \"=\"*50)\n",
                    "print(\"ðŸŽ‰ Training session completed successfully!\")\n",
                    "print(\"ðŸ’¡ All checkpoints and models are saved to Google Drive\")\n",
                    "print(\"ðŸš€ Your Khmer OCR model is ready for deployment!\")\n",
                    "\n",
                    "# Instructions for next steps\n",
                    "print(\"\\nðŸ“‹ Next Steps:\")\n",
                    "print(\"   1. Download the best model from Google Drive\")\n",
                    "print(\"   2. Test the model on new Khmer text images\")\n",
                    "print(\"   3. Deploy the model in your application\")\n",
                    "print(\"   4. Monitor performance and fine-tune if needed\")"
                ]
            }
        ],
        "metadata": {
            "accelerator": "GPU",
            "colab": {
                "collapsed_sections": [],
                "machine_shape": "hm",
                "provenance": []
            },
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.10"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Save the notebook
    with open("khmer_ocr_colab_training.ipynb", "w", encoding="utf-8") as f:
        json.dump(notebook, f, indent=2, ensure_ascii=False)
    
    print("âœ… Google Colab notebook created successfully!")
    print("ðŸ“ File: khmer_ocr_colab_training.ipynb")
    print("ðŸ”— Upload to Google Colab and run!")

if __name__ == "__main__":
    create_colab_notebook() 